<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>神经网络实践：Spaceship Titanic预测模型优化笔记 | Kalev Yang</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="在这篇博客中，我将分享在Kaggle的Spaceship Titanic竞赛中构建预测模型的完整流程，重点介绍如何通过余弦退火学习率调度器提升模型性能。
竞赛概述
Spaceship Titanic是Kaggle上的一个经典二分类问题，目标是预测乘客是否在宇宙飞船事故中被运输到另一个维度。这是一个典型的结构化数据分类问题，适合初学者入门机器学习。
模型架构与优化策略
核心模型结构
我构建了一个简单的全连接神经网络作为基础模型：
class SpaceshipNN(nn.Module):
    def __init__(self, input_size):
        super(SpaceshipNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.3)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.sigmoid(self.fc4(x))
        return x
关键优化：余弦退火学习率调度
为什么选择余弦退火？
传统的学习率调度器如ReduceLROnPlateau在验证损失停止改善时降低学习率，但这种方法可能导致训练过早收敛。余弦退火则提供了一种更平滑、周期性的学习率调整方式：
替换传统的ReduceLROnPlateau为CosineAnnealingLR
from torch.optim.lr_scheduler import CosineAnnealingLR

# 初始化优化器
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 初始化余弦退火学习率调度器
scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
实现细节：
• T_max=50：半个周期的epoch数">
    <meta name="generator" content="Hugo 0.154.3">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.efe4d852f731d5d1fbb87718387202a97aafd768cdcdaed0662bbe6982e91824.css" >




    


    
      

    

    

    
      <link rel="canonical" href="https://kalev-ovo.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E8%B7%B5spaceship-titanic%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/">
    

    
    
    <meta property="og:url" content="https://kalev-ovo.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E8%B7%B5spaceship-titanic%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/">
  <meta property="og:site_name" content="Kalev Yang">
  <meta property="og:title" content="神经网络实践：Spaceship Titanic预测模型优化笔记">
  <meta property="og:description" content="在这篇博客中，我将分享在Kaggle的Spaceship Titanic竞赛中构建预测模型的完整流程，重点介绍如何通过余弦退火学习率调度器提升模型性能。 竞赛概述 Spaceship Titanic是Kaggle上的一个经典二分类问题，目标是预测乘客是否在宇宙飞船事故中被运输到另一个维度。这是一个典型的结构化数据分类问题，适合初学者入门机器学习。
模型架构与优化策略 核心模型结构 我构建了一个简单的全连接神经网络作为基础模型：
class SpaceshipNN(nn.Module):def __init__(self, input_size):super(SpaceshipNN, self).__init__()self.fc1 = nn.Linear(input_size, 128)self.fc2 = nn.Linear(128, 64)self.fc3 = nn.Linear(64, 32)self.fc4 = nn.Linear(32, 1)self.dropout = nn.Dropout(0.3)self.relu = nn.ReLU()self.sigmoid = nn.Sigmoid()def forward(self, x):x = self.relu(self.fc1(x))x = self.dropout(x)x = self.relu(self.fc2(x))x = self.dropout(x)x = self.relu(self.fc3(x))x = self.sigmoid(self.fc4(x))return x 关键优化：余弦退火学习率调度 为什么选择余弦退火？ 传统的学习率调度器如ReduceLROnPlateau在验证损失停止改善时降低学习率，但这种方法可能导致训练过早收敛。余弦退火则提供了一种更平滑、周期性的学习率调整方式：
替换传统的ReduceLROnPlateau为CosineAnnealingLR from torch.optim.lr_scheduler import CosineAnnealingLR# 初始化优化器optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)# 初始化余弦退火学习率调度器scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) 实现细节： • T_max=50：半个周期的epoch数">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-25T00:41:20+00:00">
    <meta property="article:modified_time" content="2025-11-25T00:41:20+00:00">
    <meta property="article:tag" content="神经网络">
    <meta property="article:tag" content="预测模型">
    <meta property="article:tag" content="机器学习">

  <meta itemprop="name" content="神经网络实践：Spaceship Titanic预测模型优化笔记">
  <meta itemprop="description" content="在这篇博客中，我将分享在Kaggle的Spaceship Titanic竞赛中构建预测模型的完整流程，重点介绍如何通过余弦退火学习率调度器提升模型性能。 竞赛概述 Spaceship Titanic是Kaggle上的一个经典二分类问题，目标是预测乘客是否在宇宙飞船事故中被运输到另一个维度。这是一个典型的结构化数据分类问题，适合初学者入门机器学习。
模型架构与优化策略 核心模型结构 我构建了一个简单的全连接神经网络作为基础模型：
class SpaceshipNN(nn.Module):def __init__(self, input_size):super(SpaceshipNN, self).__init__()self.fc1 = nn.Linear(input_size, 128)self.fc2 = nn.Linear(128, 64)self.fc3 = nn.Linear(64, 32)self.fc4 = nn.Linear(32, 1)self.dropout = nn.Dropout(0.3)self.relu = nn.ReLU()self.sigmoid = nn.Sigmoid()def forward(self, x):x = self.relu(self.fc1(x))x = self.dropout(x)x = self.relu(self.fc2(x))x = self.dropout(x)x = self.relu(self.fc3(x))x = self.sigmoid(self.fc4(x))return x 关键优化：余弦退火学习率调度 为什么选择余弦退火？ 传统的学习率调度器如ReduceLROnPlateau在验证损失停止改善时降低学习率，但这种方法可能导致训练过早收敛。余弦退火则提供了一种更平滑、周期性的学习率调整方式：
替换传统的ReduceLROnPlateau为CosineAnnealingLR from torch.optim.lr_scheduler import CosineAnnealingLR# 初始化优化器optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)# 初始化余弦退火学习率调度器scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) 实现细节： • T_max=50：半个周期的epoch数">
  <meta itemprop="datePublished" content="2025-11-25T00:41:20+00:00">
  <meta itemprop="dateModified" content="2025-11-25T00:41:20+00:00">
  <meta itemprop="wordCount" content="165">
  <meta itemprop="keywords" content="神经网络,预测模型,机器学习">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="神经网络实践：Spaceship Titanic预测模型优化笔记">
  <meta name="twitter:description" content="在这篇博客中，我将分享在Kaggle的Spaceship Titanic竞赛中构建预测模型的完整流程，重点介绍如何通过余弦退火学习率调度器提升模型性能。 竞赛概述 Spaceship Titanic是Kaggle上的一个经典二分类问题，目标是预测乘客是否在宇宙飞船事故中被运输到另一个维度。这是一个典型的结构化数据分类问题，适合初学者入门机器学习。
模型架构与优化策略 核心模型结构 我构建了一个简单的全连接神经网络作为基础模型：
class SpaceshipNN(nn.Module):def __init__(self, input_size):super(SpaceshipNN, self).__init__()self.fc1 = nn.Linear(input_size, 128)self.fc2 = nn.Linear(128, 64)self.fc3 = nn.Linear(64, 32)self.fc4 = nn.Linear(32, 1)self.dropout = nn.Dropout(0.3)self.relu = nn.ReLU()self.sigmoid = nn.Sigmoid()def forward(self, x):x = self.relu(self.fc1(x))x = self.dropout(x)x = self.relu(self.fc2(x))x = self.dropout(x)x = self.relu(self.fc3(x))x = self.sigmoid(self.fc4(x))return x 关键优化：余弦退火学习率调度 为什么选择余弦退火？ 传统的学习率调度器如ReduceLROnPlateau在验证损失停止改善时降低学习率，但这种方法可能导致训练过早收敛。余弦退火则提供了一种更平滑、周期性的学习率调整方式：
替换传统的ReduceLROnPlateau为CosineAnnealingLR from torch.optim.lr_scheduler import CosineAnnealingLR# 初始化优化器optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)# 初始化余弦退火学习率调度器scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6) 实现细节： • T_max=50：半个周期的epoch数">

      
      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Kalev Yang
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l mw7 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">神经网络实践：Spaceship Titanic预测模型优化笔记</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-11-25T00:41:20Z">November 25, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100-l"><h2 id="在这篇博客中我将分享在kaggle的spaceship-titanic竞赛中构建预测模型的完整流程重点介绍如何通过余弦退火学习率调度器提升模型性能">在这篇博客中，我将分享在Kaggle的Spaceship Titanic竞赛中构建预测模型的完整流程，重点介绍如何通过余弦退火学习率调度器提升模型性能。</h2>
<h2 id="竞赛概述">竞赛概述</h2>
<p>Spaceship Titanic是Kaggle上的一个经典二分类问题，目标是预测乘客是否在宇宙飞船事故中被运输到另一个维度。这是一个典型的结构化数据分类问题，适合初学者入门机器学习。</p>
<h2 id="模型架构与优化策略">模型架构与优化策略</h2>
<h3 id="核心模型结构">核心模型结构</h3>
<p>我构建了一个简单的全连接神经网络作为基础模型：</p>
<pre tabindex="0"><code>class SpaceshipNN(nn.Module):
    def __init__(self, input_size):
        super(SpaceshipNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.3)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.sigmoid(self.fc4(x))
        return x
</code></pre><h3 id="关键优化余弦退火学习率调度">关键优化：余弦退火学习率调度</h3>
<p>为什么选择余弦退火？
传统的学习率调度器如ReduceLROnPlateau在验证损失停止改善时降低学习率，但这种方法可能导致训练过早收敛。余弦退火则提供了一种更平滑、周期性的学习率调整方式：</p>
<h3 id="替换传统的reducelronplateau为cosineannealinglr">替换传统的ReduceLROnPlateau为CosineAnnealingLR</h3>
<pre tabindex="0"><code>from torch.optim.lr_scheduler import CosineAnnealingLR

# 初始化优化器
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 初始化余弦退火学习率调度器
scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
</code></pre><h3 id="实现细节">实现细节：</h3>
<p>• T_max=50：半个周期的epoch数</p>
<p>• eta_min=1e-6：最小学习率</p>
<p>• 每个epoch后调用scheduler.step()更新学习率</p>
<h3 id="训练流程优化">训练流程优化</h3>
<pre tabindex="0"><code>def train_with_cosine_annealing(model, train_loader, val_loader, epochs=100):
    for epoch in range(epochs):
        # 训练阶段
        model.train()
        for batch in train_loader:
            # 前向传播、反向传播、参数更新
            ...
        
        # 验证阶段
        model.eval()
        with torch.no_grad():
            ...
        
        # 更新学习率（关键步骤）
        scheduler.step()
        
        # 记录和打印训练信息
        if epoch % 10 == 0:
            print(f&#39;Epoch {epoch} | Train Loss: {train_loss:.4f} | &#39;
                  f&#39;Val Loss: {val_loss:.4f} | LR: {current_lr:.6f}&#39;)
</code></pre><h2 id="实验结果">实验结果</h2>
<p>通过引入余弦退火学习率调度，模型表现有了显著提升：</p>
<ul>
<li>
<p>初始得分：0.80360</p>
</li>
<li>
<p>优化后最佳得分：0.80734（提升约0.47%）</p>
</li>
<li>
<p>训练稳定性：明显改善，避免了学习率过早衰减的问题</p>
</li>
</ul>
<h3 id="技术要点总结">技术要点总结</h3>
<ol>
<li>
<p>学习率调度的重要性：合适的学习率调度策略可以显著影响模型收敛和最终性能。</p>
</li>
<li>
<p>余弦退火的优势：</p>
<ul>
<li>
<p>平滑的学习率变化减少训练震荡</p>
</li>
<li>
<p>周期性重启帮助跳出局部最优</p>
</li>
<li>
<p>无需手动设置patience参数</p>
</li>
</ul>
</li>
<li>
<p>实践建议：</p>
<ul>
<li>
<p>根据数据集大小调整T_max参数</p>
</li>
<li>
<p>监控学习率变化确保合理范围</p>
</li>
<li>
<p>结合早停法防止过拟合</p>
</li>
</ul>
</li>
</ol>
<h3 id="进一步优化方向">进一步优化方向</h3>
<p>虽然余弦退火带来了明显改进，但仍有进一步优化的空间：</p>
<ol>
<li>带热身的余弦退火：使用CosineAnnealingWarmRestarts实现更复杂的调度策略</li>
<li>模型架构优化：尝试更深的网络或注意力机制</li>
<li>特征工程：深入挖掘数据特征的相关性</li>
</ol>
<h3 id="结语">结语</h3>
<p>通过这个项目，我深刻体会到学习率调度在深度学习中的重要性。余弦退火作为一种简单而有效的技术，值得在各类分类任务中尝试和应用。机器学习不仅是模型架构的竞赛，更是对训练细节的精心打磨。</p>
<p>完整代码和详细实现请参考我的Kaggle笔记本：https://www.kaggle.com/code/kalevyang/notebooke4132da306</p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">神经网络</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">预测模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">机器学习</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div></article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="https://kalev-ovo.github.io/" >
    &copy;  Kalev Yang 2026 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
